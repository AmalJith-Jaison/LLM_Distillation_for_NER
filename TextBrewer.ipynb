{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaForCausalLM, LlamaTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from textbrewer import GeneralDistiller, TrainingConfig, DistillationConfig\n",
    "import os\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b75fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDataset(Dataset):\n",
    "    #filepath=r\"D:\\models\\training_data\"\n",
    "    def __init__(self, filepath, tokenizer, max_length=128):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.lines = [line.strip() for line in f if line.strip()]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.lines[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids = enc[\"input_ids\"].squeeze()\n",
    "        attention_mask = enc[\"attention_mask\"].squeeze()\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": input_ids.clone()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a6f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = \"cuda\"   You can change to 'cuda' if GPU available\n",
    "\n",
    "teacher_path =r\"D:\\models\\llama8b\"\n",
    "teacher = AutoModelForCausalLM.from_pretrained(\n",
    "    teacher_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# teacher = AutoModelForCausalLM.from_pretrained(teacher_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf538d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_path =  r\"D:\\models\\llama3b\"\n",
    "student = AutoModelForCausalLM.from_pretrained(\n",
    "    student_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_template = \"\"\"You are a helpful assistant. return awb number, :\n",
    "‚ùó Important:\n",
    "- Do **not** use given examples for any fields\n",
    "- Do **not** use any parts from the prompt as fields\n",
    "- Only return the final JSON object.\n",
    "- Do **not** add any explanation, markdown formatting, or code.\n",
    "- Do **not** include backticks (```) or language tags like ```json.\n",
    "- Do **not** generate Python or any other code.\n",
    "- If data is missing, return `null`, but do not fabricate.\n",
    "- Output must be valid and clean JSON.\n",
    "- Only take values from the given mail\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "Subject: {subject}\n",
    "From: {from_}\n",
    "To: {to}\n",
    "Body:{body}\n",
    "######################################################\n",
    " You are assisting the Airline Cargo team in extracting specific business-critical entities from customer emails. Each email consists of a subject and body, both delimited by triple backticks (```).\n",
    " \n",
    "You must return a JSON array of dictionaries, each representing one AWB (Air Waybill) entry with its corresponding entities. The structure should follow these rules:\n",
    "Return a JSON array of dictionaries.\n",
    "Each dictionary must include the extracted AWB and its associated entities.\n",
    "Omit any fields not found in the email; DO NOT assume values.\n",
    " \n",
    "The final output must be pure JSON: no explanations, no backticks, no extra text.\n",
    "The json output should appear as per the following format\n",
    "[\n",
    "    {{\n",
    "        \"AWB\": \"\",\n",
    "        \"FlightNo\": \"\",\n",
    "        \"Departure-date\": \"\",\n",
    "        \"total-pieces\": ,\n",
    "        \"pieces@dimensions\": [\"\"],\n",
    "        \"dimension-unit\": [\"\"],\n",
    "        \"Weight\": ,\n",
    "        \"weight-unit\": \"\",\n",
    "        \"special-instruction\": \"\",\n",
    "        \"commodity-description\": \"\",\n",
    "        \"product-code\": \"\",\n",
    "        \"Source\": \"\",\n",
    "        \"Destination\": \"\"\n",
    "    }}\n",
    "]\n",
    "AWB (Air Waybill):\n",
    "Must be 11-digit numbers starting with valid airline prefixes: <AWB_PREFIX>.\n",
    "May be referred to as \"MAWB\" or \"GUIA\".\n",
    "Remove hyphens or spaces.\n",
    "One dictionary per AWB; multiple AWBs = multiple dictionaries.\n",
    " \n",
    "FlightNo:\n",
    "Must start with valid carrier codes: <AIRLINE_PREFIX>.\n",
    "Format: airline code + number\n",
    "do not take the date value for the flight number if there is flight date attached with flight number (eg KE706/18APR in this only take KE706)\n",
    "if no values are found keep as null\n",
    " \n",
    "Departure-date:\n",
    "Extract in YYYY-MM-DD format.\n",
    "If given as a range like 23/24/07, choose the latest date (i.e., 2025-07-24).\n",
    "If given as a relative day (e.g., \"next Monday\"), assume today's date is 2025-03-22 (Saturday) and resolve accordingly.\n",
    " \n",
    "total-pieces:\n",
    "Integer value representing total cargo pieces.\n",
    " \n",
    "pieces@dimensions:\n",
    "Format: list like [\"2@24x17x9\"].\n",
    "May appear as pcs x l x b x h or pcs @ l x b x h.\n",
    "Extract all combinations; prioritize individual dimensions over total.\n",
    " \n",
    "dimension-unit:\n",
    "Supported units: \"CM\", \"M\", \"IN\", \"OTH\".\n",
    "Provide as a list matching the sequence of pieces@dimensions.\n",
    " \n",
    "Weight:\n",
    "If individual weights are given, compute the total.\n",
    "Use chargeable weight (CW) or gross weight (G/W) if explicitly mentioned.\n",
    "If weight is embedded in piece-dimension combos, extract accordingly.\n",
    " \n",
    "weight-unit:\n",
    "Supported values: \"KG\", \"KGS\", \"LBS\", \"OTH\".\n",
    " \n",
    "special-instruction:\n",
    "Extract any special handling notes.\n",
    "Always translate to English.\n",
    " \n",
    "commodity-description:\n",
    "Free text describing the goods.\n",
    "Always translate to English.\n",
    " \n",
    "product-code:\n",
    "If not explicitly given, infer from commodity description:\n",
    "\"GEN\" for general cargo\n",
    "\"HAZ\" for hazardous materials\n",
    "\"DG\" for dangerous goods\n",
    " \n",
    "Source / Destination:\n",
    "Extract from IATA codes in formats like EWR-OME (EWR = Source, OME = Destination).\n",
    "Do not assume source location from sender‚Äôs location or from flight number.\n",
    " \n",
    "Must Translate all extracted text into  json.\n",
    "Do not fabricate missing values.\n",
    "Always return a clean JSON output only ‚Äî no markdown, no backticks, no wrapping text.\n",
    "Must Not generate anything other than the base json file. Do NOT generate any code.\n",
    "Only the output is required do not generate anything else\n",
    " \n",
    "since there is a json format given generate just as the json format. Do not generate in loop. if it start to generate in loop stop the generation.\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d90b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptResponseDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, prompt_template, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt_template = prompt_template  # <-- store the prompt\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for item in data:\n",
    "            subject = item.get(\"subject\", \"\")\n",
    "            from_ = item.get(\"from\", \"\")\n",
    "            to = item.get(\"to\", \"\")\n",
    "            body = item.get(\"body\", \"\")\n",
    "\n",
    "            self.samples.append({\n",
    "                \"subject\": subject,\n",
    "                \"from_\": from_,\n",
    "                \"to\": to,\n",
    "                \"body\": body\n",
    "            })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        subject = sample[\"subject\"]\n",
    "        from_ = sample[\"from_\"]\n",
    "        to = sample[\"to\"]\n",
    "        body = sample[\"body\"]\n",
    "\n",
    "        # Use the shared prompt template\n",
    "        prompt = self.prompt_template.format(subject=subject, from_=from_, to=to, body=body)\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": inputs[\"input_ids\"].squeeze(0).clone(),\n",
    "            \"prompt\": prompt,\n",
    "            \"subject\": subject,\n",
    "            \"from_\": from_,\n",
    "            \"to\": to,\n",
    "            \"body\": body\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e11136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"D:\\distil\\structured_outputs.json\"\n",
    "dataset = PromptResponseDataset(file_path, tokenizer, prompt_template)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "# dataset = KnowledgeDataset(r\"C:\\Users\\211369\\Desktop\\program\\distil\\airline.txt\", tokenizer)\n",
    "# train_dataloader = DataLoader(dataset,batch_size=2,shuffle=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d66c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "for batch in dataloader:\n",
    "    prompts = batch[\"prompt\"]  # batch[\"prompt\"] is a list of strings\n",
    "    for prompt in prompts:\n",
    "        print(\"üì® Prompt:\\n\", prompt)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29318e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_response(prompt: str, max_tokens=512):\n",
    "#     inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = teacher.generate(**inputs, max_new_tokens=max_tokens)\n",
    "#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# response = generate_response(prompt)\n",
    "# generated_text = response[len(prompt):].strip()\n",
    "\n",
    "# print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ab481",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "batch = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    t_out = teacher(**batch)\n",
    "s_out = student(**batch)\n",
    "print(t_out.logits.shape)  # [B, T, V]\n",
    "print(s_out.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23c217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccb3e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(student.parameters(), lr= 5e-5)\n",
    "\n",
    "train_config = TrainingConfig(\n",
    "    device='cuda',                  # or 'cpu' if no GPU\n",
    "    output_dir='./saved_model',\n",
    "    log_dir='./log',\n",
    ")\n",
    "\n",
    "distill_config = DistillationConfig(\n",
    "    temperature=2.0,\n",
    "    kd_loss_type='ce',              # 'ce' for KLDiv, 'mse' for regression\n",
    "    kd_loss_weight=1.0,             # use 1.0 for pure distillation\n",
    "    hard_label_weight=1.0,          # set 0.0 to ignore ground-truth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab25e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_adaptor(batch, model_output):\n",
    "    return {\n",
    "        \"logits\": model_output.logits,\n",
    "        \"labels\": batch.get(\"labels\", None) \n",
    "    }\n",
    "\n",
    "\n",
    "def batch_postprocessor(batch):\n",
    "    return {\n",
    "        \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "        \"labels\": batch[\"labels\"].to(device),\n",
    "    }\n",
    "def callback(step=None, loss=None, lr=None, model=None):\n",
    "    if loss is not None:\n",
    "        print(f\"[Step {step}] Total Loss: {loss:.4f} | LR: {lr:.6f}\")\n",
    "    else:\n",
    "        print(f\"[Step {step}] Loss is None\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf84ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "distiller = GeneralDistiller(\n",
    "    train_config=train_config,\n",
    "    distill_config=distill_config,\n",
    "    model_T=teacher,\n",
    "    model_S=student,\n",
    "    adaptor_T=get_adaptor,\n",
    "    adaptor_S=get_adaptor,\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abcd7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distiller.__class__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8634bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))\n",
    "loss, loss_dict = distiller.train_on_batch(batch, {})\n",
    "print(\"Loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print(distiller.__class__)\n",
    "from types import MethodType\n",
    "\n",
    "def force_general_train(self, *args, **kwargs):\n",
    "    print(\"‚úÖ This is GeneralDistiller.train()\")\n",
    "    return GeneralDistiller.train(self, *args, **kwargs)\n",
    "\n",
    "distiller.train = MethodType(force_general_train, distiller)\n",
    "\n",
    "original_train_on_batch = distiller.train_on_batch\n",
    "\n",
    "def wrapped_train_on_batch(self, batch, args):\n",
    "    loss, loss_dict = original_train_on_batch(batch, args)\n",
    "    print(f\"[DISTILL STEP] Loss: {loss.item():.4f}\")\n",
    "    return loss, loss_dict\n",
    "\n",
    "from types import MethodType\n",
    "distiller.train_on_batch = MethodType(wrapped_train_on_batch, distiller)\n",
    "\n",
    "distiller.train_with_num_epochs(\n",
    "    optimizer=optimizer,\n",
    "    scheduler=None,\n",
    "    tqdm_disable=False,\n",
    "    dataloader=train_loader,\n",
    "    max_grad_norm=1.0,\n",
    "    num_epochs=5,\n",
    "    callback=callback,\n",
    "    batch_postprocessor=None\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57dd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model=r\"D:\\models\\op\"\n",
    "\n",
    "student.save_pretrained(distilled_model)\n",
    "tokenizer.save_pretrained(distilled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a2ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ca8456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
